{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import re\n",
    "import ntpath\n",
    "import dateutil\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL VARS\n",
    "SEARCH_DATA_SYNID = 'syn11377377'\n",
    "LOCATION_DATA_SYNID = 'syn11377380'\n",
    "WEBVISTIS_SYNID = 'syn11977096'\n",
    "ENROLLMENT_SYNTABLE = 'syn11384434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_userSearchQueries_in_htmlFormat(html_file_content):\n",
    "    '''\n",
    "    html_file_content - is the content (string) of the HTML file\n",
    "    '''\n",
    "\n",
    "    textSearches = []\n",
    "    webVisits = []\n",
    "    errorBlock = []\n",
    "    #this should be a unique search block in HTML file\n",
    "    blocks = re.findall('<div class=\"outer-cell.*?</div></div></div>',html_file_content)\n",
    "    #process each block    \n",
    "    for b in blocks:\n",
    "        textSearch,webVisit = processSearchBlock(b)\n",
    "        if textSearch is None and webVisit is None:\n",
    "            errorBlock.append(b)\n",
    "        if textSearch: textSearches.append(textSearch) \n",
    "        if webVisit: webVisits.append(webVisit)\n",
    "    webVisits_df = pd.DataFrame.from_records(webVisits,columns=('weblink', 'localTimestamp', 'location'))\n",
    "    textSearches_df = pd.DataFrame.from_records(textSearches,columns=('query', 'localTimestamp', 'location'))\n",
    "    return(webVisits_df, textSearches_df)\n",
    "\n",
    "\n",
    "def processSearchBlock(block):\n",
    "    #website_visits_match = re.match('^.+Visited.+?\">(.*)/</a><br>(.*?)</div>.+$', block)\n",
    "    webVisit       = re.match('^.*Visited.*\">(.*)</a><br>(.*?)</div.*$', block)\n",
    "    textSearch     = re.match('^.+?Searched for.+?\">(.+?)</a><br>(.*?)</div>.+$', block)\n",
    "    location       = re.match('^.*Locations.*\">(.*)</a><br></div></div></div>$', block)\n",
    "    \n",
    "    if location:\n",
    "        location = location.groups()\n",
    "    else:\n",
    "        location = ('NA',)\n",
    "    \n",
    "    if textSearch:\n",
    "        textSearch = textSearch.groups()\n",
    "        textSearch = textSearch + location\n",
    "    else:\n",
    "        textSearch = None\n",
    "    \n",
    "    if webVisit:\n",
    "        webVisit = webVisit.groups()\n",
    "        webVisit = webVisit + location\n",
    "    else:\n",
    "        webVist = None\n",
    "    return([textSearch,webVisit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extIDNA_afsID9129_date2018-08-07_time22-20-47'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fileName = '/Users/apratap/Downloads/test_takeOut_data/9129_takeout-20180807T222047Z-001.zip'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process WebSearches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _tmp_process_manually_downloaded_takeoutFile(takeoutFile):\n",
    "    \n",
    "    ## metadata extraction\n",
    "    re_match = re.match('^(.*?)_takeout-(.*?)-001.zip$', ntpath.basename(fileName)).groups()\n",
    "    extID = 'NA'\n",
    "    afsID = re_match[0]\n",
    "    consentTime = dateutil.parser.parse(re_match[1])\n",
    "    tmp_date = consentTime.strftime('%Y-%m-%d')\n",
    "    tmp_time = consentTime.strftime('%H-%M-%S')\n",
    "    prefix = 'extID%s_afsID%s_date%s_time%s' % (extID,afsID, tmp_date, tmp_time)\n",
    "\n",
    "    ###read and process the zip file\n",
    "    x = ZipFile(fileName)\n",
    "    search_files = [f for f in x.namelist() if 'Search' in f]\n",
    "    search_file = search_files[0]\n",
    "    html_file_content = x.read(search_file).decode('utf-8')\n",
    "    webVisits, searchQueries = process_userSearchQueries_in_htmlFormat(html_file_content)\n",
    "\n",
    "    #until we have a plan to handle this better \n",
    "    searchQueries = searchQueries.drop(['location'], axis=1)\n",
    "    webVisits = webVisits.drop(['location'], axis=1)\n",
    "\n",
    "    ### Run DLP API on Search Queries\n",
    "    searchQueriesList = searchQueries['query'].tolist()\n",
    "    dlp_results = DLP.run(searchQueriesList) \n",
    "\n",
    "    #integrate with queries\n",
    "    searchQueries = searchQueries.merge(dlp_results, how='left', left_on='query', right_on='searchQuery')\n",
    "    searchQueries = searchQueries.drop(['searchQuery'], axis=1)\n",
    "    searchQueries['query'][searchQueries.redacted == True ] = 'REDACTED'\n",
    "    \n",
    "    ###Upload data to synapse\n",
    "    searchQueriesFile = '%s_searchQueries.feath' % user.prefix\n",
    "    searchQueries.to_feather(searchQueriesFile)\n",
    "    tmp = syn.store(synapseclient.File(searchQueriesFile, parentId = SEARCH_DATA_SYNID))\n",
    "    os.unlink(searchQueriesFile)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Process Locations\n",
    "    location_files = [f for f in x.namelist() if 'Location History' in f]\n",
    "    if len(location_files) > 0:\n",
    "        locations = pd.read_json(x.open(location_files[0]).read())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_process_manually_downloaded_takeoutFile(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
